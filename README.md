# Diffusion Models for 2-D Geoscience & Image In-Painting

This repository contains a set of lightweight, **PyTorch + ðŸ¤— Diffusers** examples that demonstrate how to train and use Denoising Diffusion Probabilistic Models (DDPM) for both toy image data (MNIST) and real-world 2-D geoscience data sets such as COâ‚‚ plumes, SIS facies and fluvial channels.  
The project also showcases *RePaint* based in-painting / data-conditioning, making it a handy starting point for seismic tomography or geostatistical reconstruction tasks.

---

## Table of Contents
1. [Project Highlights](#project-highlights)  
2. [Directory Layout](#directory-layout)  
3. [Quick Start](#quick-start)  
4. [Training Workflows](#training-workflows)  
5. [RePaint In-painting](#repaint-in-painting)  
6. [Utilities & Visualisation](#utilities--visualisation)  
7. [Results](#results)  
8. [Troubleshooting](#troubleshooting)  
9. [Roadmap](#roadmap)  
10. [Citation](#citation) / [Licence](#licence)

---

## Project Highlights
â€¢ **Minimal code â€‘ maximum clarity** â€“ fewer than ~150 lines per experiment.  
â€¢ **Multiple data domains** â€“ MNIST digits and several geoscience pixel grids.  
â€¢ **GPU-ready** â€“ Automatically selects CUDA when available.  
â€¢ **Flexible output sizes** â€“ thanks to fully-convolutional U-Net architecture.  
â€¢ **In-painting** â€“ RePaint pipeline with configurable masks (random pixels, seismic ray paths, patches, â€¦).

---

## Directory Layout
```text
.
â”œâ”€â”€ DDPM/                    # All diffusion experiments
â”‚   â”œâ”€â”€ MNIST_examples/
â”‚   â”‚   â””â”€â”€ main_mnist.py
â”‚   â”œâ”€â”€ CO2_PLUME_examples/
â”‚   â”‚   â”œâ”€â”€ main_co2.py
â”‚   â”‚   â””â”€â”€ test_flexible_dimension.py
â”‚   â””â”€â”€ Geo_examples/
â”‚       â”œâ”€â”€ main_geo.py
â”‚       â””â”€â”€ repaint.py
â”œâ”€â”€ utility.py               # Data-loader helpers & mask generators
â”œâ”€â”€ viz.py                   # Visualisation helpers
â”œâ”€â”€ roadpath.md              # Development diary / notes
â””â”€â”€ literatures/             # (optional) reading material & papers
```
Each experiment folder also contains a `monitor/` sub-folder where training curves and generated samples are saved.

---

## Quick Start
### 1. Clone & install
```bash
# clone
$ git clone <repo-url>
$ cd <repo-dir>

# create environment (conda or venv)
$ conda create -n ddpm python=3.10 -y
$ conda activate ddpm

# install core dependencies
$ pip install torch torchvision diffusers matplotlib tqdm
```
Optional extras: `jupyter`, `notebook`, `ipykernel` if you wish to run the notebooks in `./DDPM/*/Analysis.ipynb`.

### 2. Prepare data
All geoscience data sets are stored as *PyTorch tensors* (`.pt`) and **are NOT checked into the repo**.  
Update the absolute paths inside `utility.py` to point to your local copies:
```python
file = '/path/to/geo_dataset/uncond-sis-train.pt'
```

### 3. Kick off a run
```bash
# MNIST toy example (CPU or single-GPU)
$ python DDPM/MNIST_examples/main_mnist.py

# COâ‚‚ plume (needs ~6 GB GPU mem)
$ python DDPM/CO2_PLUME_examples/main_co2.py

# Fluvial / SIS facies (64Ã—64 grids)
$ python DDPM/Geo_examples/main_geo.py
```
Outputs (loss curves + sample grids) will be written to the corresponding `monitor/` directory.

---

## Training Workflows
All scripts follow the same pattern:
1. **Load data** via one of the helper dataloaders in `utility.py` (MNIST, SIS, fluvial, COâ‚‚ plume, â€¦).  
2. **Instantiate U-Net** (`UNet2DModel`) and configure channels/blocks.  
3. **Define scheduler** â€“ `DDPMScheduler` with 1 000 steps (modifiable).  
4. **Optimise** with Adam + _MSE_ or _L1_ targets.  
5. **Monitor** â€“ after every *k* epochs images are sampled through a `DDPMPipeline` and saved.

Hyper-parameters can be changed at the top of each `main_*.py` file.

---

## RePaint In-painting
The script `DDPM/Geo_examples/repaint.py` demonstrates conditioning a pre-trained DDPM on partial observations using **RePaint**:
```bash
$ python DDPM/Geo_examples/repaint.py
```
Key components:
* `get_random_pixel_mask`, `get_penetrate_mask`, `patched_mask` in `utility.py` create binary masks.  
* A trained model checkpoint (`monitor_channel/model_50.pt`) is loaded and passed to `RePaintPipeline`.
* Output grids + side-by-side comparisons are saved via `viz.compare_repaint`.

---

## Utilities & Visualisation
* **`utility.py`** â€“ dataloaders and geometric mask generators (ray-path, patch, random pixel).  
* **`viz.py`** â€“ helper functions for tidier matplotlib grids, conditional vs generated sample comparison, etc.

---

## Results
Sample outputs (generated digits, COâ‚‚ plume reconstructions, patch masks, â€¦) are committed as PNGs inside each `monitor*/` or `roadpath_images/` folder for quick inspection.

---

## Troubleshooting
â€¢ **White / saturated backgrounds** â€“ make sure you rescale tensors to `[-1,1]` during training and clamp back to `[0,1]` for visualisation (see comments in `roadpath.md`).  
â€¢ **CUDA OOM** â€“ reduce `batch_size` or `block_out_channels`; try `mixed_precision="fp16"`.

---

## Roadmap
See `roadpath.md` for the full development log.  Planned items:
1. Hyper-parameter sweep for better geological textures.  
2. DDIM & score-based variants for faster sampling.  
3. 3-D volume extension.

---

## Citation
If you build on this code, please cite:
```text
@misc{ddpm_geoscience,
  author       = {Your Name},
  title        = {Diffusion Models for Geoscience In-painting},
  year         = {2024},
  howpublished = {GitHub},
  url          = {https://github.com/<your-repo>}
}
```

## Licence
[MIT](LICENSE) Â© 2024 Your Name